{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* cuda 12.1\n",
    "* pytorch 2.3.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microsoft Phi 3 mini (4bn, 4k tokens context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uses the following prompt template\n",
    "><|system|>  \n",
    ">Your Role<|end|>  \n",
    "><|user|>  \n",
    ">Your Question?<|end|>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433c7859947d4ec5972c0dc8b41d266f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ```python\n",
      "def find_pairs_with_difference_two(arr):\n",
      "    arr.sort()  # Sort the array to ensure pairs are in ascending order\n",
      "    result = []\n",
      "    for i in range(len(arr) - 1):\n",
      "        if arr[i + 1] - arr[i] == 2:\n",
      "            result.append((arr[i], arr[i + 1]))\n",
      "    return result\n",
      "\n",
      "# Example usage:\n",
      "print(find_pairs_with_difference_two([1, 2, 3, 4]))  # Output: [(1, 3), (2, 4)]\n",
      "print(find_pairs_with_difference_two([4, 1, 2, 3]))  # Output: [(1, 3), (2, 4)]\n",
      "print(find_pairs_with_difference_two([1, 23, 3, 4, 7]))  # Output: [(1, 3)]\n",
      "print(find_pairs_with_difference_two([4, 3, 1, 5, 6]))  # Output: [(1, 3), (3, 5), (4, 6)]\n",
      "```\n",
      "\n",
      "This Python function `find_pairs_with_difference_two` takes a list of integers as input and returns a list of tuples, where each tuple contains a pair of integers from the input list that have a difference of 2. The function first sorts the input list to ensure that the pairs are in ascending order. It then iterates through the sorted list, checking for pairs with a difference of 2 and appending them to the result list. Finally, it returns the result list.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \n",
    "\n",
    "torch.random.manual_seed(0) \n",
    "model = AutoModelForCausalLM.from_pretrained( \n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",  \n",
    "    device_map=\"cuda\",  \n",
    "    torch_dtype=\"auto\",  \n",
    "    trust_remote_code=True,\n",
    "    attn_implementation='eager',\n",
    ") \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\") \n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Your are a python developer.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Please only provide python code as your answer for the following assignment: The objective is to return all pairs of integers from a given array of integers that have a difference of 2.  The result array should be sorted in ascending order of values.  Assume there are no duplicate integers in the array. The order of the integers in the input array should not matter. Examples  [1, 2, 3, 4]  should return [(1, 3), (2, 4)]  [4, 1, 2, 3]  should also return [(1, 3), (2, 4)]  [1, 23, 3, 4, 7] should return [(1, 3)]  [4, 3, 1, 5, 6] should return [(1, 3), (3, 5), (4, 6)]\"},\n",
    "]\n",
    "\n",
    "pipe = pipeline( \n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    ") \n",
    "\n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 500, \n",
    "    \"return_full_text\": False, \n",
    "    # \"temperature\": 0.0, \n",
    "    \"do_sample\": False, \n",
    "} \n",
    "\n",
    "output = pipe(messages, **generation_args) \n",
    "print(output[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-12-3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
