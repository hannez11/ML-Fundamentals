{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* cuda 12.1\n",
    "* pytorch 2.3.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microsoft Phi 3 mini (4bn, 4k tokens context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uses the following prompt template\n",
    "><|system|>  \n",
    ">Your Role<|end|>  \n",
    "><|user|>  \n",
    ">Your Question?<|end|>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f9bafdfe3f496988ddf0096179c603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A large language model is a sophisticated artificial intelligence system that uses machine learning algorithms to understand, generate, and translate human language with high accuracy and fluency.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \n",
    "\n",
    "torch.random.manual_seed(0) \n",
    "model = AutoModelForCausalLM.from_pretrained( \n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",  \n",
    "    device_map=\"cuda\",  \n",
    "    torch_dtype=\"auto\",  \n",
    "    trust_remote_code=True,\n",
    "    attn_implementation='eager',\n",
    ") \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\") \n",
    "\n",
    "prompt = \"Give me a one sentence introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Limit your answer to one sentence.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "pipe = pipeline( \n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    ") \n",
    "\n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 500, \n",
    "    \"return_full_text\": False, \n",
    "    # \"temperature\": 0.0, \n",
    "    \"do_sample\": False, \n",
    "} \n",
    "\n",
    "output = pipe(messages, **generation_args) \n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen 2 (0.5b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The large language model is a machine learning model that can generate text based on input data. It can generate text that is similar to the input data, but it can also generate new text based on the input data. The model can also generate text that is not related to the input data, but it can still generate new text based on the input data. The model can also generate text that is not generated by the input data, but it can still generate new text based on the input data. The model can also generate text that is not generated by the input data, but it can still generate new text based on the input data. The model can also generate text that is not generated by the input data, but it can still generate new text based on the input data. The model can also generate text that is not generated by the input data, but it can still generate new text based on the input data. The model can also generate text that is not generated by the input data, but it can still generate new text based on the input data. The model can also generate text that is not generated by the input data, but it can still generate new text based on the input data. The model can also generate text that is not generated by the input data, but it can still generate new text based on the input data. The model can also generate text that is not generated by the input data, but it can still generate new text based on the input data. The model can also generate text that is not generated by the input data, but it can still generate new text based on the input data. The model can also generate text that is not generated by the input data, but it can still generate new text based on the input data. The model can also generate text that is not generated by the input data, but it can still generate new text based on the input data. The model can also generate text that is not generated by the input data, but it can still generate new text based on the input data. The model can also generate text that is not generated by the input data, but it can still generate new text based on the input data. The model can also generate text that is not generated by the input data, but it can still generate new text based on the input data. The model can also generate text that is not generated by the input data, but it can still generate new text based on the input data. The model can also generate text that is not generated by the input data, but it can still generate new'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "# Now you do not need to add \"trust_remote_code=True\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2-0.5B\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B\")\n",
    "\n",
    "# Instead of using model.chat(), we directly use model.generate()\n",
    "# But you need to use tokenizer.apply_chat_template() to format your inputs as shown below\n",
    "prompt = \"Give me a one sentence introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Limit your answer to one sentence.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Directly use generate() and tokenizer.decode() to get the output.\n",
    "# Use `max_new_tokens` to control the maximum output length.\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-12-3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
